{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECBM E4040 Assignment 1, Task 4: Questions\n",
    "1) Cross entropy is a metric that measures the \"distance\" between two distributions, why can it be used in calculating the loss of softmax classifier? \n",
    "\n",
    "   Your answer: **[Cross entropy: E=-∑yilog(pj), Softmax loss: L=-∑yilog(sj), if pj = sj,the output of Softmax loss and cross entropy are the same. When using the softmax function as the output, pj=sj, thus it can be used to calculate the loss of softmax classifier. ]**\n",
    "\n",
    "\n",
    "2) How does binary SVM classifier deal with multi-class classification problem? In general, there are two ways.Please describe both training process and inference process of different methods.\n",
    "\n",
    "   Your answer: **[Two ways: 1-vs-all and 1-vs-1. \n",
    "   1. 1-vs-all: Say there are C classes, we need to set C binary SVMs. \n",
    "   For a unlabed data, it need to go thorough all SVMs, and the result is the corresponding class which has the max output in all C SVMs. \n",
    "   In training, each of the SVMs sets one class as a class and the rest of the classes as another class, and trys to find the hyperplane between the class and the rest of the classes.\n",
    "   \n",
    "   2. 1-vs-1: Say there are C classes, we need to set SVMs between every two different classes, which is C(C-1)/2 of SVMs, and get C(C-1)/2 results. \n",
    "   For a unlabeled data, it needs to go thorough all SVMs, and get C(C-1)/2 results, then vote in all results to get C(C-1)/2 max sub-results, then add up these sub-results, the result is the class that has most number of sub-results. \n",
    "   In training, set C(C-1)/2 pairs of training data corresponding to different pairs of classes, and train them with different SVMs. ]**\n",
    "   \n",
    "\n",
    "3) What are the pros and cons of SVM compared with MLP?\n",
    "\n",
    "   Your answer: **[\n",
    "   pros：SVM is a convex optimization problem, thus won't face the local minima problem MLP has.\n",
    "         SVMs can sort classification or regresion problem with less training data.\n",
    "         It has a strong fundamental of mathematical theories, requires less experience in the implementation comparing to the MLP.\n",
    "         \n",
    "   cons: For large amount of input data, it need to do large-scale high-order matrix computation, which has a high demand for computation and storage ability.\n",
    "         It becomes more complex when solving multi-class problems.]**\n",
    "   \n",
    "   \n",
    "\n",
    "4) Why is the ReLU activation function used the most often in neural networks for computer vision?\n",
    "\n",
    "   Your answer: **[As its gradients is none-satuation, it conveges much faster in SGD comparing to other actvation function; Its implementation is simple, it does not need expensive computation like exp() in other functions; It won't have vanishing or exploding gradients problem.]**\n",
    "   \n",
    "   \n",
    "5) Describe your best model in the implementation of the two-layer neural network. Describe your starting point, how you tuned  hyperparameters, which stategies you used to improve the network, show the results of intermediate and the final steps.\n",
    "\n",
    "   Your answer: **[Accuracy: 0.509 best model: \n",
    "   hidden_dim=160, num_classes=10, reg=1e-4, weight_scale=1e-3,num_epoch = 21,batch_size = 600,learning_rate = 1e-3\n",
    "   My starting point is the same as the default given hyperparameters. First I increased the hidden dimension, the accuracy increased a little bit, than with current best hidden dimension, I increased the number of epoch to around 30, the accuracy increased at first, but later seems overfitting after around 22th epoch, so I set num_epoch=21. Then I increased learning rate, at 1e-3 the accuracy achieved more than 50%.\n",
    "Results: (complete results is shown in task2)\n",
    "number of batches for training: 81\n",
    "\n",
    "epoch 1: valid acc = 0.287, new learning rate = 0.00095\n",
    "\n",
    "epoch 2: valid acc = 0.388, new learning rate = 0.0009025\n",
    "\n",
    "epoch 3: valid acc = 0.426, new learning rate = 0.000857375\n",
    "\n",
    "epoch 4: valid acc = 0.439, new learning rate = 0.0008145062499999999\n",
    "\n",
    "epoch 5: valid acc = 0.45, new learning rate = 0.0007737809374999998\n",
    "\n",
    "epoch 6: valid acc = 0.451, new learning rate = 0.0007350918906249997\n",
    "\n",
    "epoch 7: valid acc = 0.462, new learning rate = 0.0006983372960937497\n",
    "\n",
    "epoch 8: valid acc = 0.467, new learning rate = 0.0006634204312890621\n",
    "\n",
    "epoch 9: valid acc = 0.475, new learning rate = 0.000630249409724609\n",
    "\n",
    "epoch 10: valid acc = 0.478, new learning rate = 0.0005987369392383785\n",
    "\n",
    "epoch 11: valid acc = 0.478, new learning rate = 0.0005688000922764595\n",
    "\n",
    "epoch 12: valid acc = 0.497, new learning rate = 0.0005403600876626365\n",
    "\n",
    "epoch 13: valid acc = 0.498, new learning rate = 0.0005133420832795047\n",
    "\n",
    "epoch 14: valid acc = 0.501, new learning rate = 0.00048767497911552944\n",
    "\n",
    "epoch 15: valid acc = 0.494, new learning rate = 0.00046329123015975297\n",
    "\n",
    "epoch 16: valid acc = 0.499, new learning rate = 0.0004401266686517653\n",
    "\n",
    "epoch 17: valid acc = 0.518, new learning rate = 0.00041812033521917703\n",
    "\n",
    "epoch 18: valid acc = 0.5, new learning rate = 0.00039721431845821814\n",
    "\n",
    "epoch 19: valid acc = 0.513, new learning rate = 0.0003773536025353072\n",
    "\n",
    "epoch 20: valid acc = 0.511, new learning rate = 0.0003584859224085418\n",
    "\n",
    "epoch 21: valid acc = 0.52, new learning rate = 0.0003405616262881147\n",
    "test acc: 0.509\n",
    "0.509]**\n",
    "   \n",
    "\n",
    "6) **Cross validation** is a technique used to prove the generalization ability of a model and can help you find a robust set of hyperparameters. Please describe the implementation details of **k-fold cross validation**.\n",
    "\n",
    "   Your answer: **[For a classification or refression problem, suppose there are N avaliable models M={M1,M2,..Mi,...MN},k-fold cros  validation sets 1/k of the training set as test set, train k times and test k times for each model, the error rate is the average, and the best model is the one with lowest average error rate.\n",
    "   Implementation:\n",
    "   1. Devide the training set into k seperate sub-set, say there are totally m training examples, each sub-set has m/k examples, the sub-sets are called S={S1,S2,..Sj,..Sk}\n",
    "   2.Each time, take a Mi from M, select all but one Sj from S, use the selected k-1 numbers of Si to train Mi to get hypothesis function Hij, then use Sj to test, and get an experience error.\n",
    "   3. Totally for a Mi, we can get k numbers of experience errors, thus the experience error for Mi is the average of all k experience errors.\n",
    "   4. Chooese the best model: the one with minimum experience error. Lastly train with all S,and get error Hi. ]**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
